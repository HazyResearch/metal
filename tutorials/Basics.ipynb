{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics Tutorial for Snorkel MeTaL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to introduce the basic pipeline, classes, and debugging/analysis tools in Snorkel MeTaL. In this notebook, we'll investigate a single-task problem with synthetic data, with an emphasis on the basic design concepts of MeTaL. Check out the `tutorials/` directory to see how this basic pipeline extends to the full _multi-task_ setting, and other topics.\n",
    "\n",
    "This tutorial consists of four steps, following the basic [data programming](https://arxiv.org/abs/1605.07723) pipeline as in [Snorkel](snorkel.stanford.edu):\n",
    "1. **Load Data:** In the _weakly supervised_ setting, we only have access to unlabeled data points `X`, matrix of noisy labels `L`, and dev/test labels `Y`*\n",
    "2. **Train Label Model:** The purpose of the `LabelModel` is to estimate the unknown accuracies of the labeling functions, _without access to `Y`_, and then use this to denoise and combine them into a set of _probabilistic training labels_.\n",
    "3. **Train End Model:** We can then use these training labels to supervise a discriminative classifier!\n",
    "4. **Evaluate:** We evaluate this model on a held-out test set\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to make sure that the `metal/` directory is on our Python path. If the following cell runs without an error, you're all set. If not, make sure that you've installed `snorkel-metal` with pip or conda (or that you've added the repo to your path if you're running from source; for example, running `source add_to_path.sh` from the repository root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../metal')\n",
    "import metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in a Snorkel MeTaL application is preparing your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept 1: Required Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, MeTaL makes use of the following types of data (n = # examples, m = # labeling functions):\n",
    "* X: an \\[n\\]-dim iterable of end model inputs (e.g., feature vectors or encoded sentences for an RNN)\n",
    "* Y: an \\[n\\]-dim numpy.ndarray of target labels ($Y \\in \\{1,...,k\\}^n$)\n",
    "* L: an \\[n,m\\] scipy.sparse matrix of noisy labels ($L \\in \\{0,...,k\\}^{n \\times m}$, with label 0 reserved for abstentions\n",
    "\n",
    "And optionally (for use with some debugging/analysis tools):\n",
    "* D: an \\[n\\]-dim iterable of human-readable examples (e.g., sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use pre-generated synthetic data, where each example is a bag of words drawn from a different gaussian distribution for each class from a 1000-word vocabulary, and our features are simply the 1000-dimensional vector of counts for each word. For some excellent resources on how to write labeling functions, see the Snorkel tutorials at https://github.com/HazyResearch/snorkel/tree/master/tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/basics_tutorial.pkl\", 'rb') as f:\n",
    "    X, Y, L, D = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "Y.shape\n",
    "L.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to divide your data into splits, you can do so with the provided utility function. We split our data 80/10/10 into train/dev/test, stratifying by the labels in `Y` to ensure a similar class balance in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.utils import split_data\n",
    "\n",
    "Xs, Ys, Ls, Ds = split_data(X, Y, L, D, splits=[0.8, 0.1, 0.1], stratify_by=Y, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Label Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeTaL comes with a number of analysis tools for investigating your label matrix. \n",
    "For example, you can view summary statistics for each labeling function.\n",
    "If you have a dev set with gold labels, you can view the empirical accuracy of your functions, in addition to the core summary statistics such as coverage, overlaps, and conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from metal.analysis import lf_summary\n",
    "\n",
    "lf_summary(Ls[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in more graphical investigative tools, you can take a look at the Visualization tutorial.  \n",
    "Once you're satisfied with your label matrices, it's time to aggregate the labels using the label model and train an end model with the resulting probabilistic labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Config dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Snorkel MeTaL models are driven by configuration dictionaries, or config dicts; that is, all settings for running the given model are stored in a single dictionary, `self.config`. \n",
    "\n",
    "When a model is initialized or the train_model() method is called, any extra keyword arguments passed by the user will be used to update the config dict. Then, throughout the class's methods, whenever a setting needs to be looked up, it is pulled from this dictionary, rather than required as a keyword. This has a number of benefits:\n",
    "\n",
    "1. Default values are stored in only one place, the default config dict, rather than in multiple method signatures.\n",
    "1. Code maintenance is simplified, since fewer keyword arguments need to be passed up and down the call stack.\n",
    "2. Logging is simplified, since the complete settings for a given model can be logged by simply writing the config dict (a python dictionary) to file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When constructing a LabelModel, the only required argument is `k`, the cardinality of the task. For example, k=2 means a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.label_model import LabelModel\n",
    "label_model = LabelModel(k=2, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only required argument to `LabelModel.train_model()` is a label matrix. All other keyword arguments are optional. \n",
    "\n",
    "For example, if you know the class_balance of our data, you can pass that in as a list or array; alternatively, you may pass in the target labels of your dev set (`Y_dev`) and have the class balance estimated from that. If neither of these is provided, the classes are assumed to be uniformly distributed.\n",
    "\n",
    "Any other keyword arguments (e.g., the number of epochs (`n_epochs`), print frequency (`print_every`), learning rate (`lr`), L2 regularization (`l2`), optimizer type (`optimizer`), etc.) will be used to update the config dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "label_model.train_model(Ls[0], Y_dev=Ys[1], n_epochs=500, print_every=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the quality of our label model on our dev set as a sanity check, but we'll see if we can do better in Step 3 by using the predictions of the label model to train a discriminative model over a larger feature set than just the outputs of these ten labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = label_model.score((Ls[1], Ys[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the score method reports accuracy, but you can also give it a list of metrics to report. Depending on your application, F1 score may be a more natural metric to use, especially if you have large class imbalance. Our synthetic data, for example, has an approximate 25/75 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = label_model.score((Ls[1], Ys[1]), metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our trained `LabelModel` outperforms the baseline of taking the majority vote label by approximately 4 points in accuracy and 3 F1 points on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.label_model.baselines import MajorityLabelVoter\n",
    "\n",
    "mv = MajorityLabelVoter(seed=123)\n",
    "scores = mv.score((Ls[1], Ys[1]), metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Classifier Base Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `LabelModel` and `EndModel` in MeTaL are children of the parent `Classifier` class. The `Classifier` class defines three important methods related to making predictions:\n",
    "* `predict_proba()`: Returns an [n, k+1] numpy array of probabilistic labels (k+1 because of the option to abstain)\n",
    "* `predict()`: Returns an [n]-dim numpy array of hard (integer) labels in {0,...,k+1}\n",
    "* `score()`: Returns the score (default: accuracy) of the predictions with respect to target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the data programming paradigm, we will use our label model's soft predictions as training labels for a discriminative end model. Because we want probabilistic labels, we use the `predict_proba()` method here. The predictions are in the form of an [n,k] np.ndarray of floats summing to 1.0 in each row, where `Y_ps[i,j]` is the predicted probability of the $i$th data point having true label $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train_ps stands for \"Y[labels]_train[split]_p[redicted]s[oft]\"\n",
    "Y_train_ps = label_model.predict_proba(Ls[0])\n",
    "Y_train_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we may want to analyze our LabelModel's predictions so that we better understand what our model is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the confusion matrix to get a sense for class size, error rates, and commonly confused classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.analysis import confusion_matrix\n",
    "\n",
    "Y_dev_p = label_model.predict(Ls[1])\n",
    "\n",
    "cm = confusion_matrix(Ys[1], Y_dev_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If matplotlib is installed, we can also use some of the visualization tools provided in the `contrib/` directory to plot the label distributions.  \n",
    "First, we observe that our problem has a severe class imbalance, which our classifier captures in a general sense.  \n",
    "Second, we can look at how confident our model is in its predictions for class 1 and see that most have probabilities very close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from metal.contrib.visualization.analysis import (\n",
    "        plot_predictions_histogram, \n",
    "        plot_probabilities_histogram,\n",
    "    )\n",
    "\n",
    "    plot_predictions_histogram(Y_dev_p, Ys[1], title=\"Label Distribution\")\n",
    "\n",
    "    Y_dev_ps = label_model.predict_proba(Ls[1])\n",
    "    plot_probabilities_histogram(Y_dev_ps[:,0], title=\"Probablistic Label Distribution\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"The tools in contrib/visualization/ require matplotlib. Try `conda/pip install matplotlib`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct and train our `EndModel` on the predictions from the LabelModel. Here we describe the construction of a single-task end model. For details on the construction of multi-task end models, refer to the multi-task tutorial.\n",
    "\n",
    "The `EndModel` consists of three components: an *input layer*, zero or more *middle layers*, and a *head layer*, with each layer consisting of a torch.nn.Module followed by various optional additional operators (e.g., a ReLU nonlinearity, batch normalization, and/or dropout). \n",
    "\n",
    "* **Input layer**:\n",
    "The input module is an IdentityModule by default, which simply accepts arbitrary-length feature vectors from X as torch.Tensors and passes them on to the next module in the network. If, however, you would like provide your data in some other format, you may pass in a custom nn.Module which maps your input type to torch.Tensors (for example, if your data points are images, you may pass in a ResNet as your input module).\n",
    "\n",
    "* **Middle layers**\n",
    "The middle layers are layers between the input layer and head layer. Middle modules are nn.Linear by default.\n",
    "\n",
    "* **Head layer**:\n",
    "The head layer is the final layer, consisting of an nn.Linear module (plus a softmax operation when making predictions).\n",
    " \n",
    "When initiated, the EndModel requires a list of output dimensions. \n",
    "The first element is the output dimension of the input module (which is equal to the dimensionality of your feature vectors if using the default Identitymodule).\n",
    "The last element is the cardinality of the classifier.\n",
    "The elements in the middle define the number and dimensionality of any middle layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from metal.end_model import EndModel\n",
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "end_model = EndModel([1000,10,2], seed=123, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once initiated, the network structure is printed so you can confirm that it captured the architecture you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we pass in our unlabeled training data, our predicted soft labels from the label aggregator, a dev set X and Y if we want to checkpoint the best model seen so far as we train, and any other keyword arguments that you'd like to update in the config dict. For this synthetic problem, the trends in our data are fairly simple, so our model will begin to overfit very quickly. If we do begin to overfit and see our dev score go down, however, the best model seen at any epoch will be restored at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "end_model.train_model((Xs[0], Y_train_ps), valid_data=(Xs[1], Ys[1]), l2=0.1, batch_size=256, \n",
    "                n_epochs=5, print_every=1, validation_metric='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a trained EndModel, we can now evaluate performance on our held-out test set, observing a substantial boost in F1 score over using the LabelModel directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Label Model:\")\n",
    "score = label_model.score((Ls[2], Ys[2]), metric=['precision', 'recall', 'f1'])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"End Model:\")\n",
    "score = end_model.score((Xs[2], Ys[2]), metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you go! If you'd like to learn about some of the other features in Snorkel MeTaL, including support for multi-task learning (MTL), hyperparameter tuning, and synthetic data generation, give some of the other tutorial notebooks a try!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metal]",
   "language": "python",
   "name": "conda-env-metal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
